Purpose
Centralizing OpenAI requests in Adroit allows us to track and manage costs, token usage, requests, and traffic across ongoing experiments across different repositories and services. This also allows for a centralized location for preprocessing and postprocessing of OpenAI requests if needed.

To view metrics for current experiments: visit go/openaidash

Experiment Setup
Experiment Config
Each experiment that calls OpenAI online will need to check in an experiment config here.

The experiment config contains the following information:

  # The name of the team that the experiment(s) belong to
  example_team:
    # The name of the experiment
    example_experiment:
      # The expected cost per day of the experiment
      expected_cost_per_day_dollars: 1
      # The budget limit for the experiment. This is the cost of the experiment before ramping
      # (i.e. cost to set up and complete a/b testing).
      # After a/b testing and prior to ramping, we will seek further cost approval and update the budget limit.
      budget_limit_dollars: 100
      # The expected queries per second of the experiment.
      # This is to help prevent and identify rate limiting
      expected_queries_per_second: 1
      # The max tokens for the open_ai request.
      # This is an open_ai parameter and helps prevent overuse of tokens.
      max_tokens: 10
Please append your experiment's configuration to the config file, and attach evidence of budget approval in the PR. Budget approval should be done by your manager / Qi He.

Cost Estimates
To estimate your experiment's cost, first estimate the total tokens that each request uses. You can make sample requests to estimate your token size or you use OpenAI's tokenizer. Then, refer to OpenAI's token pricing.

Personal Usage
Refer to ChatGPT Policy and Usage Instructions

Local Development
To experiment with OpenAI locally, use a Nextdoor account OpenAI token and the openai library to make requests to OpenAI models. You can request to be added to the Nextdoor OpenAI account in the #generative-ai slack channel.

To hit the adroit openai endpoint via the monolith, follow these steps:

Clone adroit and create the openai conda env

Run bootstrap/create_env.sh conda_envs/chatgpt-2023-06 in ~/src/adroit

conda activate chatgpt-2023-06

Start the local adroit OpenAI flask server using the latest OpenAI flavor. The latest flavor can be found in flavors.yaml.

Run FLAVOR={latest_openai_flavor} bash run_flask.sh.

i.e. FLAVOR=openAIA bash run_flask.sh

Databricks Notebook Development
You can fetch the OpenAI Token in a Databricks Notebook via dbutils.secrets.get(scope="openai", key="openai-key"). openai.organization should be set to org-YqSpGFZWZlCtSNiwxTDWpXgN, which corresponds to the nextdoor-offline OpenAI org. This allows us to separate the online and offline OpenAI rate limits.

If making lots of requests, please utilize the Shared GPT Notebook. This allows us to track offline OpenAI usage metrics. The shared notebook is managed via Terraform in Github.

The shared notebook provides a utility function make_gpt_inferences_mp which parallelizes requests, implements retries, and tracks metrics in Datadog which can be viewed in go/openaidash under the Offline Metrics section.

An example notebook can be found here.

Making Requests Online
Use the adroit client to call the OpenAI adroit endpoint. All requests to OpenAI should have a fallback option in case the OpenAI requests fails.

example request:

ac = client.get_adroit_client()
chatgpt_response = await ac.get_chatgpt_response_async(
    team_name="notifications",
    experiment_name="generate_subject",
    model=GPTModels.GPT_3,
    max_tokens=100,
    messages=[
        {"role": "system", "content": sitevar_dict["system_message"]},
        {"role": "user", "content": chatgpt_user_message},
    ],
)
subject = str(chatgpt_response["choices"][0]["message"]["content"])
The supported OpenAI models are defined in GPTModels:

class GPTModels(enum.Enum):
    GPT_3 = "gpt-3.5-turbo"
    GPT_4 = "gpt-4"
    TEXT_DAVINCI_003 = "text-davinci-003"
    # fine-tuned model for pRemove classification. See details here: http://go/gpt_premove
    P_REMOVE = "ada:ft-nextdoor:premove-v2-2023-03-17-07-55-50"
    # fine-tuned model for pUncivil classification. See details here: http://go/gpt_puncivil
    P_UNCIVIL = "ada:ft-nextdoor:puncivil-v1-2023-03-17-06-55-30"
    # fine-tuned model for pToS classification, using HH prevalence work. See details here: http://go/pToS
    P_ToS = "ada:ft-nextdoor:custom-hh-removal-npos-v1-2023-05-01-20-59-41"
Observability and Alerts
Dashboards
General metrics for each experiment related to token usage, costs, request counts, errors, and latencies can be found in go/openaidash.

For additional custom metrics, it is up to the experiment owner to log the metrics and create the dashboards. i.e. Magic Text Metrics

Latency and Error Alerts
Latency and error alerts are currently disabled for OpenAI due to the API's inconsistent performance. Empirical latency for the OpenAI call have been 500ms to several seconds. Latency is most influenced by the number of completion tokens. We have also seen various API connection errors and rate limiting errors. OpenAI has no SLA for these metrics.

Costs and Rate Limits
Should an experiment spend an unexpected amount in a week or approach the budget limit, the team that owns the experiment will get paged.

Should an experiment exceed the budget limit, CoreML and the team that owns the experiment will get paged.

The openai_blocklist sitevar can be used to blocklist any experiments that are unexpectedly incurring costs or triggering rate limits.

Handling User Generated Content (UGC) Prompt Injections
TBD

Ideas:

Put instructions in System Prompt rather than User Prompt for ChatGPT
Use special separator character/token to distinguish system instructions vs user input
Run MRRC model on output to detect if the post is Harmful/Hurtful